"""
Webç•Œé¢ - ä½¿ç”¨Streamlitåˆ›å»ºäº¤äº’å¼é£æ ¼è¿ç§»åº”ç”¨
è¿è¡Œ: streamlit run web_interface.py
"""

import streamlit as st

# é¡µé¢é…ç½®å¿…é¡»æ˜¯ç¬¬ä¸€ä¸ªStreamlitå‘½ä»¤
st.set_page_config(
    page_title="ç¥ç»é£æ ¼è¿ç§»",
    page_icon="ğŸ¨",
    layout="wide"
)

# ä¿®å¤PyTorchå’ŒStreamlitçš„å…¼å®¹æ€§é—®é¢˜
import os

# è®¾ç½®ç¯å¢ƒå˜é‡æ¥é¿å…PyTorchå’ŒStreamlitçš„å†²çª
os.environ['TORCH_FORCE_CPU_ONNX_EXPORT'] = '1'
os.environ['STREAMLIT_BROWSER_GATHER_USAGE_STATS'] = 'false'

# ç¦ç”¨PyTorchçš„æŸäº›åŠŸèƒ½ä»¥é¿å…ä¸Streamlitå†²çª
import warnings
warnings.filterwarnings('ignore', category=UserWarning, module='torch')

# å»¶è¿Ÿå¯¼å…¥torchä»¥é¿å…åˆå§‹åŒ–é—®é¢˜
@st.cache_resource
def load_torch():
    import torch
    torch.set_num_threads(1)  # é™åˆ¶çº¿ç¨‹æ•°ä»¥é¿å…å†²çª
    return torch

torch = load_torch()

from PIL import Image
import io
import tempfile
from neural_style import NeuralStyleTransfer
from config import Config
from utils import im_convert
import numpy as np

def display_image_with_aspect_ratio(image, caption, max_width=400):
    """
    æ˜¾ç¤ºå›¾åƒå¹¶ä¿æŒå®½é«˜æ¯”
    
    Args:
        image: PIL Imageå¯¹è±¡
        caption: å›¾åƒæ ‡é¢˜
        max_width: æœ€å¤§æ˜¾ç¤ºå®½åº¦
    """
    # è®¡ç®—æ˜¾ç¤ºå°ºå¯¸
    w, h = image.size
    if w > max_width:
        display_width = max_width
        display_height = int(h * max_width / w)
    else:
        display_width = w
        display_height = h
    
    # åˆ›å»ºä¸€ä¸ªè°ƒæ•´åçš„å›¾åƒç”¨äºæ˜¾ç¤º
    display_image = image.resize((display_width, display_height), Image.Resampling.LANCZOS)
    
    st.image(display_image, caption=caption)
    return display_image

# æ ‡é¢˜
st.title("ğŸ¨ ç¥ç»é£æ ¼è¿ç§»")
st.markdown("---")

# ä¾§è¾¹æ é…ç½®
st.sidebar.header("âš™ï¸ å‚æ•°è®¾ç½®")

# åŸºæœ¬å‚æ•°
max_size = st.sidebar.slider("å›¾åƒæœ€å¤§å°ºå¯¸", 256, 1024, 512, 64)
iterations = st.sidebar.slider("è¿­ä»£æ¬¡æ•°", 100, 5000, 1000, 100)
style_weight = st.sidebar.number_input("é£æ ¼æƒé‡", 1e3, 1e8, 1e6, format="%.0e")
content_weight = st.sidebar.number_input("å†…å®¹æƒé‡", 0.1, 10.0, 1.0, 0.1)
learning_rate = st.sidebar.number_input("å­¦ä¹ ç‡", 0.001, 0.01, 0.003, step=0.001, format="%.3f")

# é«˜çº§é€‰é¡¹
st.sidebar.subheader("ğŸ”§ é«˜çº§é€‰é¡¹")
preserve_colors = st.sidebar.checkbox("ä¿æŒåŸå§‹é¢œè‰²")
tv_weight = st.sidebar.number_input("æ€»å˜åˆ†æƒé‡", 0.0, 100.0, 0.0, 0.1)
save_intermediate = st.sidebar.checkbox("ä¿å­˜ä¸­é—´ç»“æœ")

# ä¸»ç•Œé¢
col1, col2, col3 = st.columns(3)

with col1:
    st.subheader("ğŸ“· å†…å®¹å›¾åƒ")
    content_file = st.file_uploader("ä¸Šä¼ å†…å®¹å›¾åƒ", type=['jpg', 'jpeg', 'png'], key="content")
    
    if content_file is not None:
        content_image = Image.open(content_file)
        display_image_with_aspect_ratio(content_image, "å†…å®¹å›¾åƒ", 300)

with col2:
    st.subheader("ğŸ­ é£æ ¼å›¾åƒ")
    style_file = st.file_uploader("ä¸Šä¼ é£æ ¼å›¾åƒ", type=['jpg', 'jpeg', 'png'], key="style")
    
    if style_file is not None:
        style_image = Image.open(style_file)
        display_image_with_aspect_ratio(style_image, "é£æ ¼å›¾åƒ", 300)

with col3:
    st.subheader("ğŸ–¼ï¸ ç»“æœå›¾åƒ")
    result_placeholder = st.empty()
    
    # æ˜¾ç¤ºåˆå§‹æç¤º
    if content_file is None or style_file is None:
        result_placeholder.info("ğŸ‘† è¯·å…ˆä¸Šä¼ å†…å®¹å›¾åƒå’Œé£æ ¼å›¾åƒ")
    else:
        result_placeholder.success("âœ… å›¾åƒå·²å‡†å¤‡å°±ç»ªï¼Œç‚¹å‡»ä¸‹æ–¹æŒ‰é’®å¼€å§‹é£æ ¼è¿ç§»")

# å¤„ç†æŒ‰é’®
st.markdown("---")
col_btn1, col_btn2, col_btn3 = st.columns([1, 1, 1])

with col_btn2:
    if st.button("ğŸš€ å¼€å§‹é£æ ¼è¿ç§»", type="primary", use_container_width=True):
        if content_file is not None and style_file is not None:            # åˆ›å»ºè¿›åº¦æ¡
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            try:
                # æ˜¾ç¤ºå¼€å§‹å¤„ç†çš„æç¤º
                result_placeholder.info("ğŸš€ æ­£åœ¨åˆå§‹åŒ–ç¥ç»é£æ ¼è¿ç§»...")
                  # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
                with tempfile.TemporaryDirectory() as temp_dir:
                    content_path = os.path.join(temp_dir, "content.jpg")
                    style_path = os.path.join(temp_dir, "style.jpg")
                    
                    # è·å–åŸå§‹å†…å®¹å›¾åƒçš„å°ºå¯¸
                    original_content_size = content_image.size  # (width, height)
                    
                    content_image.save(content_path)
                    style_image.save(style_path)
                    
                    # åˆ›å»ºé…ç½®
                    config = Config()
                    config.content_path = content_path
                    config.style_path = style_path
                    config.output_dir = temp_dir
                    config.max_size = max_size
                    config.iterations = iterations
                    config.style_weight = style_weight
                    config.content_weight = content_weight
                    config.learning_rate = learning_rate
                    config.preserve_colors = preserve_colors
                    config.total_variation_weight = tv_weight
                    config.save_intermediate = save_intermediate
                    config.show_every = max(1, iterations // 20)  # æ˜¾ç¤º20æ¬¡æ›´æ–°
                      # æ‰§è¡Œé£æ ¼è¿ç§»
                    status_text.text("ğŸ”„ æ­£åœ¨åŠ è½½ç¥ç»ç½‘ç»œæ¨¡å‹...")
                    nst = NeuralStyleTransfer(config)
                    nst.load_images()
                    nst.setup_losses()
                    
                    status_text.text("ğŸ¨ å¼€å§‹é£æ ¼è¿ç§»ä¼˜åŒ–è¿‡ç¨‹...")
                      # è‡ªå®šä¹‰ä¼˜åŒ–å¾ªç¯ä»¥æ˜¾ç¤ºè¿›åº¦
                    optimizer = torch.optim.Adam([nst.target_img], lr=config.learning_rate)
                      # è·å–å†…å®¹å›¾åƒçš„åŸå§‹å°ºå¯¸
                    content_width, content_height = original_content_size  # PILçš„sizeæ˜¯(width, height)
                    
                    for iteration in range(1, config.iterations + 1):
                        optimizer.zero_grad()
                        
                        target_features = nst.vgg_features(nst.target_img)
                        
                        # å†…å®¹æŸå¤±
                        content_loss = nst.content_loss(target_features[config.content_layer])
                        content_loss *= config.content_weight
                        
                        # é£æ ¼æŸå¤±
                        style_loss = 0
                        for layer, loss_info in nst.style_losses.items():
                            if layer in target_features:
                                layer_loss = loss_info['loss'](target_features[layer])
                                style_loss += layer_loss * loss_info['weight']
                        style_loss *= config.style_weight
                        
                        # æ€»å˜åˆ†æŸå¤±
                        tv_loss = 0
                        if nst.tv_loss:
                            tv_loss = nst.tv_loss(nst.target_img)
                        
                        total_loss = content_loss + style_loss + tv_loss
                        total_loss.backward()
                        optimizer.step()
                        
                        # æ›´æ–°è¿›åº¦
                        progress = iteration / config.iterations
                        progress_bar.progress(progress)
                        
                        # å®æ—¶æ›´æ–°æ˜¾ç¤º
                        if iteration % config.show_every == 0:
                            status_text.text(f"è¿­ä»£ {iteration}/{config.iterations} - æŸå¤±: {total_loss.item():.4f}")
                            
                            # æ˜¾ç¤ºä¸­é—´ç»“æœ
                            current_result = nst.target_img.clone()
                            if config.preserve_colors:
                                from models import preserve_color
                                current_result = preserve_color(nst.content_img, current_result)
                            
                            result_image = im_convert(current_result)
                            result_pil = Image.fromarray((result_image * 255).astype(np.uint8))
                            
                            # è°ƒæ•´ç»“æœå›¾åƒå°ºå¯¸ä¸å†…å®¹å›¾åƒä¸€è‡´
                            result_pil = result_pil.resize((content_width, content_height), Image.Resampling.LANCZOS)
                              # æ˜¾ç¤ºä¸­é—´ç»“æœï¼Œä¿æŒåŸå§‹å°ºå¯¸æ¯”ä¾‹
                            result_placeholder.empty()
                            with result_placeholder.container():
                                display_image_with_aspect_ratio(result_pil, f"å®æ—¶ç»“æœ (è¿­ä»£ {iteration})", 400)# æœ€ç»ˆç»“æœ
                    final_result = nst.target_img.clone()
                    if config.preserve_colors:
                        from models import preserve_color
                        final_result = preserve_color(nst.content_img, final_result)
                    
                    result_image = im_convert(final_result)
                    result_pil = Image.fromarray((result_image * 255).astype(np.uint8))
                    
                    # è°ƒæ•´æœ€ç»ˆç»“æœå›¾åƒå°ºå¯¸ä¸å†…å®¹å›¾åƒä¸€è‡´
                    result_pil = result_pil.resize((content_width, content_height), Image.Resampling.LANCZOS)
                    
                    # æ˜¾ç¤ºæœ€ç»ˆç»“æœï¼Œä¿æŒåŸå§‹å°ºå¯¸æ¯”ä¾‹
                    result_placeholder.empty()
                    with result_placeholder.container():
                        display_image_with_aspect_ratio(result_pil, "âœ¨ æœ€ç»ˆç»“æœ", 400)
                    
                    status_text.text("âœ… é£æ ¼è¿ç§»å®Œæˆï¼")
                    progress_bar.progress(1.0)
                    
                    # æä¾›ä¸‹è½½æŒ‰é’®
                    img_buffer = io.BytesIO()
                    result_pil.save(img_buffer, format='JPEG')
                    img_buffer.seek(0)
                    
                    st.download_button(
                        label="ğŸ’¾ ä¸‹è½½ç»“æœ",
                        data=img_buffer.getvalue(),
                        file_name="neural_style_result.jpg",
                        mime="image/jpeg"
                    )
                    
            except Exception as e:
                st.error(f"é”™è¯¯: {e}")
                
        else:
            st.warning("è¯·å…ˆä¸Šä¼ å†…å®¹å›¾åƒå’Œé£æ ¼å›¾åƒï¼")

# ç¤ºä¾‹å›¾åƒ
st.markdown("---")
st.subheader("ğŸ¨ ç¤ºä¾‹å›¾åƒ")

example_col1, example_col2 = st.columns(2)

with example_col1:
    st.markdown("**å†…å®¹å›¾åƒç¤ºä¾‹**")
    if st.button("ä½¿ç”¨ç¤ºä¾‹å†…å®¹å›¾åƒ"):
        if os.path.exists("data/content/content.jpg"):
            st.session_state.example_content = Image.open("data/content/content.jpg")
            st.image(st.session_state.example_content, caption="ç¤ºä¾‹å†…å®¹å›¾åƒ")

with example_col2:
    st.markdown("**é£æ ¼å›¾åƒç¤ºä¾‹**")
    if st.button("ä½¿ç”¨ç¤ºä¾‹é£æ ¼å›¾åƒ"):
        if os.path.exists("data/style/style.jpg"):
            st.session_state.example_style = Image.open("data/style/style.jpg")
            st.image(st.session_state.example_style, caption="ç¤ºä¾‹é£æ ¼å›¾åƒ")

# è¯´æ˜
st.markdown("---")
with st.expander("ğŸ“– ä½¿ç”¨è¯´æ˜"):
    st.markdown("""
    ### å¦‚ä½•ä½¿ç”¨ç¥ç»é£æ ¼è¿ç§»ï¼š
    
    1. **ä¸Šä¼ å›¾åƒ**ï¼šåˆ†åˆ«ä¸Šä¼ å†…å®¹å›¾åƒå’Œé£æ ¼å›¾åƒ
    2. **è°ƒæ•´å‚æ•°**ï¼šåœ¨å·¦ä¾§è°ƒæ•´è¿­ä»£æ¬¡æ•°ã€æƒé‡ç­‰å‚æ•°
    3. **å¼€å§‹å¤„ç†**ï¼šç‚¹å‡»"å¼€å§‹é£æ ¼è¿ç§»"æŒ‰é’®
    4. **æŸ¥çœ‹ç»“æœ**ï¼šç­‰å¾…å¤„ç†å®Œæˆå¹¶ä¸‹è½½ç»“æœ
    
    ### å‚æ•°è¯´æ˜ï¼š
    
    - **å›¾åƒæœ€å¤§å°ºå¯¸**ï¼šè¾ƒå¤§çš„å°ºå¯¸ä¼šäº§ç”Ÿæ›´é«˜è´¨é‡çš„ç»“æœï¼Œä½†éœ€è¦æ›´å¤šæ—¶é—´
    - **è¿­ä»£æ¬¡æ•°**ï¼šæ›´å¤šè¿­ä»£é€šå¸¸äº§ç”Ÿæ›´å¥½çš„ç»“æœ
    - **é£æ ¼æƒé‡**ï¼šæ§åˆ¶é£æ ¼è¿ç§»çš„å¼ºåº¦
    - **å†…å®¹æƒé‡**ï¼šæ§åˆ¶å†…å®¹ä¿æŒçš„ç¨‹åº¦
    - **ä¿æŒåŸå§‹é¢œè‰²**ï¼šä¿æŒå†…å®¹å›¾åƒçš„åŸå§‹é¢œè‰²ä¿¡æ¯
    - **æ€»å˜åˆ†æƒé‡**ï¼šå‡å°‘ç»“æœä¸­çš„å™ªå£°
    
    ### æç¤ºï¼š
    
    - å»ºè®®ä»è¾ƒå°‘çš„è¿­ä»£æ¬¡æ•°å¼€å§‹æµ‹è¯•
    - é«˜åˆ†è¾¨ç‡å›¾åƒéœ€è¦æ›´å¤šGPUå†…å­˜
    - å¯ä»¥å°è¯•ä¸åŒçš„å‚æ•°ç»„åˆè·å¾—æœ€ä½³æ•ˆæœ
    """)

# é¡µè„š
st.markdown("---")
st.markdown("Made with â¤ï¸ using Streamlit and PyTorch")
